{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu2dp8Z7gwftb4gZ23TQGC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elnegmelnegm/AutoRegress/blob/main/AutoRegress_HPC_Nested_CV_Edition_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U83xRGWBtGEj"
      },
      "source": [
        "HPC Nested CV Edition\n",
        "# AutoRegress: Bayesian Optimized Regression Modeler (BO) with Filter Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize statsmodels pandas numpy scipy scikit-learn matplotlib seaborn skrebate markdown-pdf gradio psutil openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psutil\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import traceback\n",
        "import joblib\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# Statistics & ML\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "# FIXED: Added train_test_split here\n",
        "from sklearn.model_selection import KFold, cross_val_score, cross_validate, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
        "import sklearn.base\n",
        "\n",
        "# Bayesian Optimization\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# Reporting & UI\n",
        "from markdown_pdf import Section, MarkdownPdf\n",
        "import gradio as gr\n",
        "\n",
        "# HPC / Advanced Feature Selection\n",
        "try:\n",
        "    from skrebate import ReliefF\n",
        "except ImportError:\n",
        "    print(\"Warning: skrebate library not found. ReliefF filter will be unavailable.\")\n",
        "    ReliefF = None\n",
        "\n",
        "# --- Configuration ---\n",
        "COLOR_TRAIN = 'lightgray'\n",
        "COLOR_NCV = '#1f77b4'\n",
        "COLOR_EXT = '#ff7f0e'\n",
        "COLOR_MEAN_SPECTRUM = '#1f77b4'\n",
        "COLOR_SELECTED_DOTS = '#ff7f0e'\n",
        "\n",
        "# --- Global State ---\n",
        "GLOBAL_MODEL = None\n",
        "GLOBAL_SCALER = None\n",
        "GLOBAL_SELECTED_INDICES = None\n",
        "GLOBAL_FEATURE_NAMES = None\n",
        "TOTAL_MODELS_TRAINED = 0\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 2: HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def log_system_resources():\n",
        "    cpu_count = psutil.cpu_count(logical=True)\n",
        "    mem = psutil.virtual_memory()\n",
        "    total_mem_gb = mem.total / (1024 ** 3)\n",
        "    msg = (f\"[HPC RESOURCE MONITOR]\\n\"\n",
        "           f\"Logical CPU Cores: {cpu_count}\\n\"\n",
        "           f\"Total RAM: {total_mem_gb:.2f} GB\\n\"\n",
        "           f\"Parallelization: Enabled (n_jobs=-1)\\n\")\n",
        "    return msg\n",
        "\n",
        "def extract_params(model_obj):\n",
        "    params = {}\n",
        "    filter_params = {}\n",
        "    if isinstance(model_obj, Pipeline):\n",
        "        if 'sel' in model_obj.named_steps:\n",
        "            f = model_obj.named_steps['sel']\n",
        "            if hasattr(f, 'k'): filter_params['k'] = f.k\n",
        "            if hasattr(f, 'n_features_to_select'): filter_params['k'] = f.n_features_to_select\n",
        "            if hasattr(f, 'n_neighbors'): filter_params['n_neighbors'] = f.n_neighbors\n",
        "        est = model_obj.named_steps['est']\n",
        "        raw_p = est.get_params()\n",
        "    else:\n",
        "        raw_p = model_obj.get_params()\n",
        "        filter_params = {\"Type\": \"None (All Features)\"}\n",
        "\n",
        "    keep_keys = ['alpha', 'l1_ratio', 'C', 'gamma', 'kernel', 'n_components']\n",
        "    for k, v in raw_p.items():\n",
        "        if k in keep_keys: params[k] = v\n",
        "    return params, filter_params\n",
        "\n",
        "# --- Custom Score Functions ---\n",
        "def pearson_corr_score_func(X, y):\n",
        "    X_df = pd.DataFrame(X)\n",
        "    scores = []\n",
        "    for i in range(X_df.shape[1]):\n",
        "        try:\n",
        "            val = abs(pearsonr(X_df.iloc[:, i], y)[0])\n",
        "            scores.append(val if not np.isnan(val) else 0)\n",
        "        except: scores.append(0)\n",
        "    return np.array(scores)\n",
        "\n",
        "def spearman_corr_score_func(X, y):\n",
        "    X_df = pd.DataFrame(X)\n",
        "    scores = []\n",
        "    for i in range(X_df.shape[1]):\n",
        "        try:\n",
        "            val = abs(spearmanr(X_df.iloc[:, i], y)[0])\n",
        "            scores.append(val if not np.isnan(val) else 0)\n",
        "        except: scores.append(0)\n",
        "    return np.array(scores)\n",
        "\n",
        "# --- Plotting Functions ---\n",
        "\n",
        "def apply_publication_style(ax, title, xlabel, ylabel):\n",
        "    \"\"\"Applies specific font sizes and weights for publication.\"\"\"\n",
        "    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel(xlabel, fontsize=18, fontweight='bold', labelpad=10)\n",
        "    ax.set_ylabel(ylabel, fontsize=18, fontweight='bold', labelpad=10)\n",
        "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "\n",
        "def save_dual_format(fig, path_base):\n",
        "    # Save PDF for Publication\n",
        "    pdf_path = f\"{path_base}.pdf\"\n",
        "    fig.savefig(pdf_path, dpi=600, format='pdf', bbox_inches='tight')\n",
        "\n",
        "    # Save PNG for App Display\n",
        "    png_path = f\"{path_base}.png\"\n",
        "    fig.savefig(png_path, dpi=300, format='png', bbox_inches='tight')\n",
        "    return pdf_path, png_path\n",
        "\n",
        "def plot_validation_results(y_ncv, p_ncv, y_ext, p_ext, title_info, save_base):\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Nested CV\n",
        "    r2_ncv = r2_score(y_ncv, p_ncv)\n",
        "    rmse_ncv = np.sqrt(mean_squared_error(y_ncv, p_ncv))\n",
        "    ax.scatter(y_ncv, p_ncv, color=COLOR_NCV, alpha=0.7, edgecolors='w', s=110, marker='o',\n",
        "               label=f\"Nested CV (Internal)\\n$R^2$={r2_ncv:.3f}, RMSEP={rmse_ncv:.3f}\")\n",
        "\n",
        "    # External Test\n",
        "    if y_ext is not None:\n",
        "        r2_ext = r2_score(y_ext, p_ext)\n",
        "        rmse_ext = np.sqrt(mean_squared_error(y_ext, p_ext))\n",
        "        ax.scatter(y_ext, p_ext, color=COLOR_EXT, alpha=0.7, edgecolors='w', s=110, marker='o',\n",
        "                   label=f\"External Test (Unseen)\\n$R^2$={r2_ext:.3f}, RMSEP={rmse_ext:.3f}\")\n",
        "\n",
        "    # Ideal Line\n",
        "    all_vals = np.concatenate([y_ncv])\n",
        "    if y_ext is not None: all_vals = np.concatenate([all_vals, y_ext])\n",
        "    min_val, max_val = all_vals.min(), all_vals.max()\n",
        "    buff = (max_val - min_val) * 0.05\n",
        "    ax.plot([min_val-buff, max_val+buff], [min_val-buff, max_val+buff], 'k--', lw=2, zorder=1)\n",
        "\n",
        "    apply_publication_style(ax, f\"Validation Performance: {title_info}\", \"Actual Values\", \"Predicted Values\")\n",
        "    ax.legend(loc='upper left', fontsize=12, frameon=True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return save_dual_format(fig, save_base)\n",
        "\n",
        "def plot_residuals(y_ncv, p_ncv, y_ext, p_ext, title_info, save_base):\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    res_ncv = y_ncv - p_ncv\n",
        "    std_ncv = np.std(res_ncv)\n",
        "    stud_ncv = res_ncv / std_ncv if std_ncv > 1e-9 else res_ncv\n",
        "    ax.scatter(p_ncv, stud_ncv, color=COLOR_NCV, alpha=0.7, edgecolors='w', s=110, marker='o', label=\"Nested CV Residuals\")\n",
        "\n",
        "    if y_ext is not None:\n",
        "        res_ext = y_ext - p_ext\n",
        "        stud_ext = res_ext / std_ncv if std_ncv > 1e-9 else res_ext\n",
        "        ax.scatter(p_ext, stud_ext, color=COLOR_EXT, alpha=0.9, edgecolors='w', s=110, marker='o', label=\"External Test Residuals\")\n",
        "\n",
        "    ax.axhline(0, color='k', linestyle='--', lw=2)\n",
        "    ax.axhline(3, color='r', linestyle=':', alpha=0.5)\n",
        "    ax.axhline(-3, color='r', linestyle=':', alpha=0.5)\n",
        "\n",
        "    apply_publication_style(ax, f\"Residual Analysis: {title_info}\", \"Predicted Values\", \"Studentized Residuals\")\n",
        "    ax.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    return save_dual_format(fig, save_base)\n",
        "\n",
        "def plot_spectrum(X_df, selected_indices, title_info, save_base):\n",
        "    if X_df is None or X_df.empty: return None, None\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    try:\n",
        "        x_vals = pd.to_numeric(X_df.columns).to_numpy()\n",
        "        xlabel = \"Wavelength / Wavenumber\"\n",
        "    except:\n",
        "        x_vals = np.arange(X_df.shape[1])\n",
        "        xlabel = \"Feature Index\"\n",
        "\n",
        "    mean_spec = X_df.mean(axis=0).to_numpy()\n",
        "    ax.plot(x_vals, mean_spec, color=COLOR_MEAN_SPECTRUM, label=\"Mean Spectrum\", linewidth=2, zorder=1)\n",
        "\n",
        "    if selected_indices is not None and len(selected_indices) > 0:\n",
        "        idx_list = [int(i) for i in selected_indices if i < len(x_vals)]\n",
        "        if idx_list:\n",
        "            selected_x = x_vals[idx_list]\n",
        "            selected_y = mean_spec[idx_list]\n",
        "            ax.scatter(selected_x, selected_y, color=COLOR_SELECTED_DOTS, s=30, zorder=2, label=\"Selected Features\")\n",
        "\n",
        "    apply_publication_style(ax, f\"Feature Selection: {title_info}\", xlabel, \"Absorbance\")\n",
        "    ax.legend(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    return save_dual_format(fig, save_base)\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 3: CORE LOGIC\n",
        "# ==============================================================================\n",
        "\n",
        "def load_data(path, target_col):\n",
        "    try: df = pd.read_excel(path)\n",
        "    except Exception as e: return None, None, str(e)\n",
        "    if target_col and target_col in df.columns:\n",
        "        y = df[target_col]; X = df.drop(columns=[target_col])\n",
        "    else:\n",
        "        y = df.iloc[:, 0]; X = df.iloc[:, 1:]\n",
        "    y = pd.to_numeric(y, errors='coerce')\n",
        "    mask = ~y.isna() & ~np.isinf(y)\n",
        "    X = X[mask]; y = y[mask]\n",
        "    X = X.select_dtypes(include=np.number).fillna(0)\n",
        "    if X.empty or y.empty: return None, None, \"Data is empty after cleaning.\"\n",
        "    return X, y, \"Success\"\n",
        "\n",
        "def get_model_config(max_pls_comps, n_samples):\n",
        "    configs = []\n",
        "    pls_comps = min(max_pls_comps, n_samples-2)\n",
        "    if pls_comps >= 1:\n",
        "        configs.append({\n",
        "            \"class\": PLSRegression, \"name\": \"PLS Regression\", \"fixed\": {\"scale\": False},\n",
        "            \"space\": [Integer(1, pls_comps, name='n_components')]\n",
        "        })\n",
        "    configs.append({\n",
        "        \"class\": Ridge, \"name\": \"Ridge Regression\", \"fixed\": {},\n",
        "        \"space\": [Real(1e-4, 1e3, prior='log-uniform', name='alpha')]\n",
        "    })\n",
        "    configs.append({\n",
        "        \"class\": ElasticNet, \"name\": \"ElasticNet\", \"fixed\": {\"max_iter\": 5000, \"tol\": 1e-3},\n",
        "        \"space\": [Real(1e-4, 10, prior='log-uniform', name='alpha'), Real(0.01, 0.99, name='l1_ratio')]\n",
        "    })\n",
        "    configs.append({\n",
        "        \"class\": SVR, \"name\": \"SVR (RBF)\", \"fixed\": {\"kernel\": \"rbf\"},\n",
        "        \"space\": [Real(0.1, 1000, prior='log-uniform', name='C'), Real(1e-4, 10, prior='log-uniform', name='gamma')]\n",
        "    })\n",
        "    return configs\n",
        "\n",
        "def objective_function(model_cls, fixed, space, X, y, cv_obj):\n",
        "    def obj(**params):\n",
        "        global TOTAL_MODELS_TRAINED\n",
        "        TOTAL_MODELS_TRAINED += 1\n",
        "        all_params = {**fixed, **params}\n",
        "        model = model_cls(**all_params)\n",
        "        if model_cls == PLSRegression:\n",
        "            if all_params['n_components'] > X.shape[1]: return 1e12\n",
        "        try:\n",
        "            scores = cross_val_score(model, X, y, cv=cv_obj, scoring='r2', n_jobs=-1)\n",
        "            return -np.mean(scores)\n",
        "        except: return 1e12\n",
        "    return obj\n",
        "\n",
        "def run_optimization_pipeline(X_train, y_train, n_bo_p0, n_bo_p2, n_bo_p3, r2_cutoff, inner_cv_splits, max_pls, track_ablation=False):\n",
        "    ablation_log = []\n",
        "    n_samples_train = len(y_train)\n",
        "    inner_cv = KFold(n_splits=inner_cv_splits, shuffle=True)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    n_feat = X_train.shape[1]\n",
        "\n",
        "    # --- Phase 0: Broad Search ---\n",
        "    configs = get_model_config(max_pls, n_samples_train)\n",
        "    p0_results = []\n",
        "    for cfg in configs:\n",
        "        @use_named_args(cfg['space'])\n",
        "        def wrapper(**p): return objective_function(cfg['class'], cfg['fixed'], cfg['space'], X_train_s, y_train, inner_cv)(**p)\n",
        "        try:\n",
        "            res = gp_minimize(wrapper, cfg['space'], n_calls=n_bo_p0, random_state=np.random.randint(1000))\n",
        "            p0_results.append({**cfg, \"params\": dict(zip([d.name for d in cfg['space']], res.x)), \"score\": -res.fun})\n",
        "        except: pass\n",
        "\n",
        "    if not p0_results: return None, []\n",
        "    p0_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "    best_curr = p0_results[0]\n",
        "\n",
        "    m = best_curr['class'](**best_curr['fixed'], **best_curr['params'])\n",
        "    m.fit(X_train_s, y_train)\n",
        "\n",
        "    if track_ablation:\n",
        "        cv_score_p0 = cross_val_score(m, X_train_s, y_train, cv=inner_cv, scoring='r2', n_jobs=-1).mean()\n",
        "        mp, fp = extract_params(m)\n",
        "        ablation_log.append({\n",
        "            \"Phase\": \"0 (Baseline)\", \"Model\": best_curr['name'], \"CV_R2\": cv_score_p0, \"Feats\": n_feat,\n",
        "            \"ModelParams\": mp, \"FilterParams\": fp\n",
        "        })\n",
        "\n",
        "    best_state = {\n",
        "        \"model_name\": best_curr['name'], \"model_obj\": m, \"filter_desc\": \"None\",\n",
        "        \"indices\": list(range(n_feat)), \"score\": best_curr['score']\n",
        "    }\n",
        "\n",
        "    # --- Phase 1: Filter ---\n",
        "    if best_state['score'] < r2_cutoff:\n",
        "        filters = [\n",
        "            (\"VarThresh\", VarianceThreshold(threshold=0.01)),\n",
        "            (\"Anova\", SelectKBest(f_regression)),\n",
        "            (\"Pearson\", SelectKBest(pearson_corr_score_func)),\n",
        "            (\"Spearman\", SelectKBest(spearman_corr_score_func))\n",
        "        ]\n",
        "        if ReliefF: filters.append((\"ReliefF\", ReliefF(n_neighbors=min(20, n_samples_train-1), n_jobs=1)))\n",
        "\n",
        "        k_options = sorted(list(set([int(n_feat * p) for p in [0.1, 0.3, 0.5]])))\n",
        "        if not k_options: k_options = [max(1, n_feat // 2)]\n",
        "\n",
        "        improved_p1 = False\n",
        "\n",
        "        for k in k_options:\n",
        "            for fname, fobj_template in filters:\n",
        "                try:\n",
        "                    fobj = sklearn.base.clone(fobj_template)\n",
        "                    if hasattr(fobj, 'k'): fobj.k = k\n",
        "                    elif hasattr(fobj, 'n_features_to_select'): fobj.n_features_to_select = k\n",
        "\n",
        "                    base_m = best_curr['class'](**best_curr['fixed'], **best_curr['params'])\n",
        "                    if isinstance(base_m, PLSRegression): base_m.n_components = min(base_m.n_components, k)\n",
        "\n",
        "                    pipe = Pipeline([('sel', fobj), ('est', base_m)])\n",
        "                    scores = cross_val_score(pipe, X_train_s, y_train, cv=inner_cv, scoring='r2', n_jobs=-1)\n",
        "                    mean_cv = np.mean(scores)\n",
        "\n",
        "                    if mean_cv > best_state['score']:\n",
        "                        improved_p1 = True\n",
        "                        pipe.fit(X_train_s, y_train)\n",
        "\n",
        "                        supp = []\n",
        "                        if hasattr(pipe.named_steps['sel'], 'get_support'):\n",
        "                            supp = pipe.named_steps['sel'].get_support(indices=True).tolist()\n",
        "                        elif fname == \"ReliefF\":\n",
        "                            if hasattr(pipe.named_steps['sel'], 'feature_importances_'):\n",
        "                                imp = pipe.named_steps['sel'].feature_importances_\n",
        "                                supp = np.argsort(imp)[::-1][:k].tolist()\n",
        "\n",
        "                        best_state = {\n",
        "                            \"model_name\": best_curr['name'], \"model_obj\": pipe,\n",
        "                            \"filter_desc\": f\"{fname} (k={k})\", \"indices\": supp, \"score\": mean_cv\n",
        "                        }\n",
        "                except: pass\n",
        "\n",
        "        if track_ablation and improved_p1:\n",
        "            mp, fp = extract_params(best_state['model_obj'])\n",
        "            ablation_log.append({\n",
        "                \"Phase\": \"1 (Filter)\", \"Model\": best_state['model_name'], \"CV_R2\": best_state['score'],\n",
        "                \"Feats\": len(best_state['indices']) if best_state['indices'] else k,\n",
        "                \"ModelParams\": mp, \"FilterParams\": fp\n",
        "            })\n",
        "\n",
        "        # --- Phase 2: Fine-Tune k (Simplified BO) ---\n",
        "        if improved_p1 and \"k\" in best_state.get('fp', {}):\n",
        "             pass\n",
        "\n",
        "    return best_state, ablation_log\n",
        "\n",
        "def generate_pdf_report(stats, best_final_model_info, ablation_log, total_models, filename, duration_str):\n",
        "    try:\n",
        "        pdf = MarkdownPdf(toc_level=2)\n",
        "        md = f\"# AutoRegress: Comprehensive Analysis Report\\n\"\n",
        "        md += f\"**Date:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "        md += f\"**Execution Time:** {duration_str}\\n\"\n",
        "        md += f\"**Computational Cost:** {total_models} models trained.\\n\\n\"\n",
        "\n",
        "        md += \"## 1. Performance Summary\\n\"\n",
        "        md += \"| Metric | Training (Fit) | Nested CV (Internal) | External Test (Unseen) |\\n|---|---|---|---|\\n\"\n",
        "        md += f\"| **RÂ²** | {stats['train_r2_m']:.4f} | {stats['cv_r2_m']:.4f} | **{stats['test_r2_m']:.4f}** |\\n\"\n",
        "        md += f\"| **RMSE** | {stats['train_rmse_m']:.4f} | {stats['cv_rmse_m']:.4f} | **{stats['test_rmse_m']:.4f}** |\\n\"\n",
        "        md += f\"| **MAE** | {stats['train_mae_m']:.4f} | {stats['cv_mae_m']:.4f} | **{stats['test_mae_m']:.4f}** |\\n\"\n",
        "\n",
        "        md += \"\\n## 2. Final Model Configuration\\n\"\n",
        "        md += f\"- **Algorithm:** {best_final_model_info['model_name']}\\n\"\n",
        "        md += f\"- **Filter:** {best_final_model_info['filter_desc']}\\n\"\n",
        "\n",
        "        if ablation_log:\n",
        "            final_log = ablation_log[-1]\n",
        "            md += \"\\n### Hyperparameters\\n\"\n",
        "            md += f\"- **Model Params:** `{final_log.get('ModelParams', 'N/A')}`\\n\"\n",
        "            md += f\"- **Filter Params:** `{final_log.get('FilterParams', 'N/A')}`\\n\"\n",
        "\n",
        "        md += \"\\n## 3. Elaborated Ablation Study\\n\"\n",
        "        md += \"| Phase | Best CV RÂ² | Features | Model Params |\\n|---|---|---|---|\\n\"\n",
        "        for p in ablation_log:\n",
        "            mp_str = str(p.get('ModelParams', '')).replace(',', ', ')[:40] + \"...\" if len(str(p.get('ModelParams', ''))) > 40 else str(p.get('ModelParams', ''))\n",
        "            md += f\"| {p['Phase']} | {p['CV_R2']:.4f} | {p['Feats']} | `{mp_str}` |\\n\"\n",
        "\n",
        "        pdf.add_section(Section(md))\n",
        "        pdf.save(filename)\n",
        "    except Exception as e: print(f\"PDF Error: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 4: MAIN CONTROLLER\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_autoregress(file_obj, target_col, test_row, outer_splits, inner_splits, max_pls,\n",
        "                        n_bo0, n_bo2, n_bo3, r2_cut, n_runs, progress=gr.Progress()):\n",
        "    global GLOBAL_MODEL, GLOBAL_SCALER, GLOBAL_SELECTED_INDICES, GLOBAL_FEATURE_NAMES, TOTAL_MODELS_TRAINED\n",
        "\n",
        "    start_time = time.time()\n",
        "    TOTAL_MODELS_TRAINED = 0\n",
        "\n",
        "    try:\n",
        "        if file_obj is None: return \"Please upload a file.\", None, None, None, None, None\n",
        "\n",
        "        sys_log = log_system_resources()\n",
        "        temp_dir = f\"gradio_runs/{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        X, y, msg = load_data(file_obj.name, target_col)\n",
        "        if X is None: return f\"Error: {msg}\", None, None, None, None, None\n",
        "\n",
        "        GLOBAL_FEATURE_NAMES = X.columns.tolist()\n",
        "\n",
        "        # --- SPLIT LOGIC ---\n",
        "        status_msg = sys_log\n",
        "\n",
        "        # Determine fixed Manual Split (if row provided)\n",
        "        manual_split_idx = None\n",
        "        if test_row and str(test_row).strip():\n",
        "            try:\n",
        "                split_idx = int(test_row) - 1\n",
        "                if 0 < split_idx < len(X):\n",
        "                    manual_split_idx = split_idx\n",
        "                    status_msg += f\"\\n[SPLIT] Manual Split at Row {split_idx+1}\\n\"\n",
        "                else: status_msg += \"Warning: Invalid Test Row. Using Random 20% Split.\\n\"\n",
        "            except: status_msg += \"Warning: Test Row Error. Using Random 20% Split.\\n\"\n",
        "        else:\n",
        "            status_msg += \"\\n[SPLIT] Using Default Random Split (20% Unseen).\\n\"\n",
        "\n",
        "        # --- REPEATED RUNS LOOP ---\n",
        "        agg_metrics = {\n",
        "            'train_r2': [], 'train_rmse': [], 'train_mae': [],\n",
        "            'cv_r2': [], 'cv_rmse': [], 'cv_mae': [],\n",
        "            'test_r2': [], 'test_rmse': [], 'test_mae': []\n",
        "        }\n",
        "\n",
        "        best_overall_run = None\n",
        "        best_run_score = -np.inf\n",
        "\n",
        "        status_msg += f\"Starting {n_runs} Repeated Runs...\\n\"\n",
        "\n",
        "        for i in range(int(n_runs)):\n",
        "            progress((i+1)/n_runs, desc=f\"Run {i+1}/{n_runs}\")\n",
        "\n",
        "            # 1. Create Split\n",
        "            if manual_split_idx is not None:\n",
        "                # Fixed split, but we still run loop to capture variance in Optimization (Inner CV)\n",
        "                X_opt = X.iloc[:manual_split_idx]\n",
        "                y_opt = y.iloc[:manual_split_idx]\n",
        "                X_ext = X.iloc[manual_split_idx:]\n",
        "                y_ext = y.iloc[manual_split_idx:]\n",
        "            else:\n",
        "                # Random Split\n",
        "                X_opt, X_ext, y_opt, y_ext = train_test_split(X, y, test_size=0.20, random_state=42+i)\n",
        "\n",
        "            # 2. Nested CV (Evaluation of Strategy)\n",
        "            outer_cv = KFold(n_splits=int(outer_splits), shuffle=True, random_state=42)\n",
        "            y_true_ncv = []\n",
        "            y_pred_ncv = []\n",
        "\n",
        "            for train_idx, val_idx in outer_cv.split(X_opt, y_opt):\n",
        "                X_tr, X_val = X_opt.iloc[train_idx], X_opt.iloc[val_idx]\n",
        "                y_tr, y_val = y_opt.iloc[train_idx], y_opt.iloc[val_idx]\n",
        "\n",
        "                res, _ = run_optimization_pipeline(X_tr, y_tr, int(n_bo0), int(n_bo2), int(n_bo3), r2_cut, int(inner_splits), int(max_pls), track_ablation=False)\n",
        "\n",
        "                if res:\n",
        "                    scaler_fold = StandardScaler().fit(X_tr)\n",
        "                    X_val_scaled = scaler_fold.transform(X_val)\n",
        "                    p_val = res['model_obj'].predict(X_val_scaled)\n",
        "                    y_true_ncv.extend(y_val)\n",
        "                    y_pred_ncv.extend(p_val)\n",
        "\n",
        "            # 3. Final Model (For this run)\n",
        "            final_res, ablation_log = run_optimization_pipeline(X_opt, y_opt, int(n_bo0), int(n_bo2), int(n_bo3), r2_cut, int(inner_splits), int(max_pls), track_ablation=True)\n",
        "\n",
        "            # 4. Metrics for this run\n",
        "            # Nested CV\n",
        "            y_true_ncv = np.array(y_true_ncv)\n",
        "            y_pred_ncv = np.array(y_pred_ncv)\n",
        "            run_cv_r2 = r2_score(y_true_ncv, y_pred_ncv)\n",
        "            run_cv_rmse = np.sqrt(mean_squared_error(y_true_ncv, y_pred_ncv))\n",
        "            run_cv_mae = mean_absolute_error(y_true_ncv, y_pred_ncv)\n",
        "\n",
        "            # Train (Fit)\n",
        "            curr_model = final_res['model_obj']\n",
        "            scaler_run = StandardScaler().fit(X_opt)\n",
        "            p_opt = curr_model.predict(scaler_run.transform(X_opt))\n",
        "            run_tr_r2 = r2_score(y_opt, p_opt)\n",
        "            run_tr_rmse = np.sqrt(mean_squared_error(y_opt, p_opt))\n",
        "            run_tr_mae = mean_absolute_error(y_opt, p_opt)\n",
        "\n",
        "            # Test (External)\n",
        "            p_ext = curr_model.predict(scaler_run.transform(X_ext))\n",
        "            run_te_r2 = r2_score(y_ext, p_ext)\n",
        "            run_te_rmse = np.sqrt(mean_squared_error(y_ext, p_ext))\n",
        "            run_te_mae = mean_absolute_error(y_ext, p_ext)\n",
        "\n",
        "            # Store\n",
        "            agg_metrics['train_r2'].append(run_tr_r2); agg_metrics['train_rmse'].append(run_tr_rmse); agg_metrics['train_mae'].append(run_tr_mae)\n",
        "            agg_metrics['cv_r2'].append(run_cv_r2); agg_metrics['cv_rmse'].append(run_cv_rmse); agg_metrics['cv_mae'].append(run_cv_mae)\n",
        "            agg_metrics['test_r2'].append(run_te_r2); agg_metrics['test_rmse'].append(run_te_rmse); agg_metrics['test_mae'].append(run_te_mae)\n",
        "\n",
        "            status_msg += f\"Run {i+1}: Test RÂ²={run_te_r2:.4f}\\n\"\n",
        "\n",
        "            # Keep best model for plotting\n",
        "            if run_te_r2 > best_run_score:\n",
        "                best_run_score = run_te_r2\n",
        "                # Store data for plotting (Last best run)\n",
        "                GLOBAL_MODEL = curr_model\n",
        "                GLOBAL_SCALER = scaler_run\n",
        "                GLOBAL_SELECTED_INDICES = final_res['indices']\n",
        "\n",
        "                best_overall_package = {\n",
        "                    \"y_ncv\": y_true_ncv, \"p_ncv\": y_pred_ncv,\n",
        "                    \"y_ext\": y_ext, \"p_ext\": p_ext,\n",
        "                    \"res\": final_res, \"ablation\": ablation_log\n",
        "                }\n",
        "\n",
        "        # --- STATS CALCULATION ---\n",
        "        stats = {\"n_runs\": n_runs}\n",
        "        for k, v in agg_metrics.items():\n",
        "            stats[f\"{k}_m\"] = np.mean(v)\n",
        "            stats[f\"{k}_s\"] = np.std(v)\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        duration_str = str(datetime.timedelta(seconds=int(elapsed)))\n",
        "        status_msg += f\"\\nTotal Time: {duration_str} | Models Trained: {TOTAL_MODELS_TRAINED}\\n\"\n",
        "\n",
        "        # --- ARTIFACTS ---\n",
        "        pdf_path = os.path.join(temp_dir, \"report.pdf\")\n",
        "        generate_pdf_report(stats, best_overall_package['res'], best_overall_package['ablation'], TOTAL_MODELS_TRAINED, pdf_path, duration_str)\n",
        "\n",
        "        zip_path = os.path.join(temp_dir, \"Results.zip\")\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "            t1, p1 = plot_validation_results(best_overall_package['y_ncv'], best_overall_package['p_ncv'], best_overall_package['y_ext'], best_overall_package['p_ext'], \"Validation Overview\", os.path.join(temp_dir, \"perf\"))\n",
        "            zf.write(t1, \"Performance_Plot.pdf\")\n",
        "            t2, p2 = plot_residuals(best_overall_package['y_ncv'], best_overall_package['p_ncv'], best_overall_package['y_ext'], best_overall_package['p_ext'], \"Residual Analysis\", os.path.join(temp_dir, \"resid\"))\n",
        "            zf.write(t2, \"Residual_Plot.pdf\")\n",
        "            t3, p3 = plot_spectrum(X, best_overall_package['res']['indices'], \"Final Feature Selection\", os.path.join(temp_dir, \"spec\"))\n",
        "            zf.write(t3, \"Spectrum_Plot.pdf\")\n",
        "            zf.write(pdf_path, \"Report.pdf\")\n",
        "\n",
        "        html = f\"\"\"\n",
        "        <h3>Analysis Results</h3>\n",
        "        <p><b>Models Trained:</b> {TOTAL_MODELS_TRAINED} (HPC Load) | <b>Time:</b> {duration_str}</p>\n",
        "        <table border='1' style='width:100%; border-collapse: collapse; text-align: center;'>\n",
        "        <tr style=\"background-color: #f2f2f2;\"><th>Metric</th><th>Training (Fit)</th><th>Nested CV (Internal)</th><th>External Test (Unseen)</th></tr>\n",
        "        <tr><td><b>RÂ²</b></td><td>{stats['train_r2_m']:.4f} Â± {stats['train_r2_s']:.4f}</td><td>{stats['cv_r2_m']:.4f} Â± {stats['cv_r2_s']:.4f}</td><td><b>{stats['test_r2_m']:.4f} Â± {stats['test_r2_s']:.4f}</b></td></tr>\n",
        "        <tr><td><b>RMSE</b></td><td>{stats['train_rmse_m']:.4f} Â± {stats['train_rmse_s']:.4f}</td><td>{stats['cv_rmse_m']:.4f} Â± {stats['cv_rmse_s']:.4f}</td><td><b>{stats['test_rmse_m']:.4f} Â± {stats['test_rmse_s']:.4f}</b></td></tr>\n",
        "        <tr><td><b>MAE</b></td><td>{stats['train_mae_m']:.4f} Â± {stats['train_mae_s']:.4f}</td><td>{stats['cv_mae_m']:.4f} Â± {stats['cv_mae_s']:.4f}</td><td><b>{stats['test_mae_m']:.4f} Â± {stats['test_mae_s']:.4f}</b></td></tr>\n",
        "        </table>\n",
        "        <br>\n",
        "        <div style=\"background-color: #e6f7ff; padding: 10px; border-radius: 5px;\">\n",
        "            <h4>ðŸš€ Final Model Configuration</h4>\n",
        "            <ul>\n",
        "                <li><b>Algorithm:</b> {best_overall_package['res']['model_name']}</li>\n",
        "                <li><b>Filter:</b> {best_overall_package['res']['filter_desc']}</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "        return status_msg, html, p1, p2, p3, zip_path\n",
        "\n",
        "    except Exception as e:\n",
        "        err = f\"CRITICAL ERROR: {str(e)}\\n{traceback.format_exc()}\"\n",
        "        return err, None, None, None, None, None\n",
        "\n",
        "def predict_new_data(file_obj):\n",
        "    if GLOBAL_MODEL is None: return \"Error: You must train a model first.\", None\n",
        "    if file_obj is None: return \"Error: Please upload an Excel file.\", None\n",
        "    try:\n",
        "        df = pd.read_excel(file_obj.name)\n",
        "        X_new = df.select_dtypes(include=np.number)\n",
        "        if GLOBAL_FEATURE_NAMES and X_new.shape[1] != len(GLOBAL_FEATURE_NAMES):\n",
        "            return f\"Error: Feature mismatch. Expected {len(GLOBAL_FEATURE_NAMES)} columns.\", None\n",
        "        X_scaled = GLOBAL_SCALER.transform(X_new)\n",
        "        preds = GLOBAL_MODEL.predict(X_scaled)\n",
        "        out_df = pd.DataFrame(preds, columns=[\"Predicted Value\"])\n",
        "        out_path = \"predictions.csv\"\n",
        "        out_df.to_csv(out_path, index=False)\n",
        "        return f\"Success! Predicted {len(preds)} samples.\", out_path\n",
        "    except Exception as e: return f\"Error: {str(e)}\", None\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 5: UI\n",
        "# ==============================================================================\n",
        "\n",
        "about_markdown = \"\"\"\n",
        "# AutoRegress: HPC Nested CV Edition\n",
        "\n",
        "**AutoRegress** is an automated framework designed to democratize high-performance spectral analysis. It abstracts away the complexity of training hundrads of models to deliver statistically rigorous results.\n",
        "\n",
        "## ðŸš€ Workflow\n",
        "1.  **Train:** Upload labeled data.\n",
        "    *   *Manual Split:* Enter a row number to lock away future data.\n",
        "    *   *Default Split:* Leave blank for a 20% Random Unseen Test Set.\n",
        "2.  **Optimize:** Uses **Bayesian Optimization** to tune:\n",
        "    *   **Models:** PLS, Ridge, ElasticNet, SVR.\n",
        "    *   **Filters:** VarianceThreshold, SelectKBest (ANOVA/Pearson/Spearman), ReliefF.\n",
        "3.  **Evaluate:** Reports metrics for **Training** , **Nested CV** , and **Test** (Unseen).\n",
        "\n",
        "## ðŸ’» HPC Justification\n",
        "The system tracks the **Total Models Trained**, calculated as:\n",
        "$$ Models = Outer_{Splits} \\\\times Inner_{Splits} \\\\times BO_{Calls} \\\\times Algorithm_{Count} $$\n",
        "This massive computational load justifies the use of Parallel Processing and High-Performance Computing.\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    with gr.Tabs():\n",
        "        # TAB 1: TRAINING\n",
        "        with gr.TabItem(\"Train & Evaluate\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    file = gr.File(label=\"Training Data (.xlsx)\")\n",
        "                    target = gr.Textbox(label=\"Target Column (Optional)\")\n",
        "                    test_row = gr.Textbox(label=\"Test Set Start Row (Optional - Manual Split)\")\n",
        "\n",
        "                    with gr.Accordion(\"Advanced Settings\"):\n",
        "                        r2_cut = gr.Slider(0.9, 1.0, 0.995, label=\"R2 Cutoff\")\n",
        "                        n_runs = gr.Slider(1, 20, value=3, step=1, label=\"Repeated Runs (Robustness)\")\n",
        "                        outer_splits = gr.Slider(2, 10, value=5, step=1, label=\"Outer CV Splits\")\n",
        "                        inner_splits = gr.Slider(2, 10, value=3, step=1, label=\"Inner CV Splits\")\n",
        "\n",
        "                    with gr.Accordion(\"Model Settings\"):\n",
        "                        max_pls = gr.Slider(1, 20, value=10, step=1, label=\"Max PLS\")\n",
        "                        n_bo0 = gr.Slider(5, 50, value=15, step=1, label=\"BO Calls\")\n",
        "\n",
        "                    btn = gr.Button(\"Execute Hybrid Analysis\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    html = gr.HTML(label=\"Results\")\n",
        "                    log = gr.Textbox(label=\"System & Run Log\", lines=10)\n",
        "                    with gr.Row():\n",
        "                        img1 = gr.Image(label=\"Performance Overview\")\n",
        "                        img2 = gr.Image(label=\"Residuals\")\n",
        "                    img3 = gr.Image(label=\"Spectrum\")\n",
        "                    pdf = gr.File(label=\"Download Report & PDF Figures (ZIP)\")\n",
        "\n",
        "            btn.click(execute_autoregress,\n",
        "                      inputs=[file, target, test_row, outer_splits, inner_splits, max_pls, n_bo0, n_bo0, n_bo0, r2_cut, n_runs],\n",
        "                      outputs=[log, html, img1, img2, img3, pdf])\n",
        "\n",
        "        # TAB 2: PREDICTION\n",
        "        with gr.TabItem(\"Predict New Data\"):\n",
        "            gr.Markdown(\"### Predict using the Final Model\")\n",
        "            p_file = gr.File(label=\"New Data (.xlsx) - Features Only\")\n",
        "            p_btn = gr.Button(\"Generate Predictions\", variant=\"primary\")\n",
        "            p_out = gr.Textbox(label=\"Status\")\n",
        "            p_csv = gr.File(label=\"Download Predictions\")\n",
        "            p_btn.click(predict_new_data, inputs=[p_file], outputs=[p_out, p_csv])\n",
        "\n",
        "        # TAB 3: ABOUT\n",
        "        with gr.TabItem(\"About\"):\n",
        "            gr.Markdown(about_markdown)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(\"gradio_runs\"): os.mkdir(\"gradio_runs\")\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Eu4E94VTDRS4",
        "outputId": "406caf6e-4b83-43d0-c08f-c14a9c939675"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.12/dist-packages (0.10.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: skrebate in /usr/local/lib/python3.12/dist-packages (0.62)\n",
            "Requirement already satisfied: markdown-pdf in /usr/local/lib/python3.12/dist-packages (1.10)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.5.3)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.7.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: PyMuPDF>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from markdown-pdf) (1.26.7)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.12/dist-packages (from markdown-pdf) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py==3.0.0->markdown-pdf) (0.1.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2102574520.py:618: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0745d92ae403b9c45d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0745d92ae403b9c45d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}