{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AutoRegress: Bayesian Optimized Regression Modeler (BO) with Filter Feature Selection"
      ],
      "metadata": {
        "id": "yVTZIsOS8tCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize statsmodels pandas numpy scipy scikit-learn matplotlib seaborn skrebate markdown-pdf gradio psutil openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psutil\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import traceback\n",
        "import joblib\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# Statistics & ML\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
        "import sklearn.base\n",
        "\n",
        "# Bayesian Optimization\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# Reporting & UI\n",
        "from markdown_pdf import Section, MarkdownPdf\n",
        "import gradio as gr\n",
        "\n",
        "# HPC / Advanced Feature Selection\n",
        "try:\n",
        "    from skrebate import ReliefF\n",
        "except ImportError:\n",
        "    print(\"Warning: skrebate library not found. ReliefF filter will be unavailable.\")\n",
        "    ReliefF = None\n",
        "\n",
        "# --- Configuration ---\n",
        "COLOR_TRAIN = 'lightgray'\n",
        "COLOR_NCV = '#1f77b4'\n",
        "COLOR_EXT = '#ff7f0e'\n",
        "COLOR_MEAN_SPECTRUM = '#1f77b4'\n",
        "COLOR_SELECTED_DOTS = '#ff7f0e'\n",
        "\n",
        "# --- Global State ---\n",
        "GLOBAL_PIPELINE = None\n",
        "GLOBAL_FEATURE_NAMES = None\n",
        "TOTAL_MODELS_TRAINED = 0\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 2: HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def log_system_resources():\n",
        "    cpu_count = psutil.cpu_count(logical=True)\n",
        "    mem = psutil.virtual_memory()\n",
        "    total_mem_gb = mem.total / (1024 ** 3)\n",
        "    msg = (f\"[HPC RESOURCE MONITOR]\\n\"\n",
        "           f\"Logical CPU Cores: {cpu_count}\\n\"\n",
        "           f\"Total RAM: {total_mem_gb:.2f} GB\\n\"\n",
        "           f\"Parallelization: Enabled (n_jobs=-1)\\n\")\n",
        "    return msg\n",
        "\n",
        "def extract_params(pipeline_obj):\n",
        "    \"\"\"Extracts parameters from the nested pipeline for reporting.\"\"\"\n",
        "    params = {}\n",
        "    filter_params = {}\n",
        "\n",
        "    # The object is always a Pipeline: [('scaler', ...), ('sel', ...), ('est', ...)]\n",
        "\n",
        "\n",
        "    steps = dict(pipeline_obj.named_steps)\n",
        "\n",
        "    # Extract Filter Params\n",
        "    if 'sel' in steps:\n",
        "        f = steps['sel']\n",
        "        if hasattr(f, 'k'): filter_params['k'] = f.k\n",
        "        elif hasattr(f, 'n_features_to_select'): filter_params['k'] = f.n_features_to_select\n",
        "        elif hasattr(f, 'n_neighbors'): filter_params['n_neighbors'] = f.n_neighbors\n",
        "        filter_params['Type'] = type(f).__name__\n",
        "    else:\n",
        "        filter_params = {\"Type\": \"None (Full Spectrum)\"}\n",
        "\n",
        "    # Extract Model Params\n",
        "    est = steps['est']\n",
        "    raw_p = est.get_params()\n",
        "    keep_keys = ['alpha', 'l1_ratio', 'C', 'gamma', 'kernel', 'n_components']\n",
        "    for k, v in raw_p.items():\n",
        "        if k in keep_keys: params[k] = v\n",
        "\n",
        "    return params, filter_params\n",
        "\n",
        "# --- Custom Score Functions ---\n",
        "def pearson_corr_score_func(X, y):\n",
        "    X_df = pd.DataFrame(X)\n",
        "    scores = []\n",
        "    for i in range(X_df.shape[1]):\n",
        "        try:\n",
        "            val = abs(pearsonr(X_df.iloc[:, i], y)[0])\n",
        "            scores.append(val if not np.isnan(val) else 0)\n",
        "        except: scores.append(0)\n",
        "    return np.array(scores)\n",
        "\n",
        "def spearman_corr_score_func(X, y):\n",
        "    X_df = pd.DataFrame(X)\n",
        "    scores = []\n",
        "    for i in range(X_df.shape[1]):\n",
        "        try:\n",
        "            val = abs(spearmanr(X_df.iloc[:, i], y)[0])\n",
        "            scores.append(val if not np.isnan(val) else 0)\n",
        "        except: scores.append(0)\n",
        "    return np.array(scores)\n",
        "\n",
        "# --- Plotting Functions ---\n",
        "def apply_publication_style(ax, title, xlabel, ylabel):\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
        "    ax.set_xlabel(xlabel, fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
        "    ax.grid(True, linestyle=':', alpha=0.6)\n",
        "\n",
        "def save_dual_format(fig, path_base):\n",
        "    pdf_path = f\"{path_base}.pdf\"\n",
        "    fig.savefig(pdf_path, dpi=300, format='pdf', bbox_inches='tight')\n",
        "    png_path = f\"{path_base}.png\"\n",
        "    fig.savefig(png_path, dpi=150, format='png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    return pdf_path, png_path\n",
        "\n",
        "def plot_validation_results(y_ncv, p_ncv, y_ext, p_ext, title_info, save_base):\n",
        "    fig, ax = plt.subplots(figsize=(8, 7))\n",
        "\n",
        "    # Nested CV\n",
        "    r2_ncv = r2_score(y_ncv, p_ncv)\n",
        "    rmse_ncv = np.sqrt(mean_squared_error(y_ncv, p_ncv))\n",
        "    ax.scatter(y_ncv, p_ncv, color=COLOR_NCV, alpha=0.6, edgecolors='w', s=80,\n",
        "               label=f\"Nested CV (Internal)\\n$R^2$={r2_ncv:.4f}, RMSEP={rmse_ncv:.4f}\")\n",
        "\n",
        "    # External Test\n",
        "    if y_ext is not None:\n",
        "        r2_ext = r2_score(y_ext, p_ext)\n",
        "        rmse_ext = np.sqrt(mean_squared_error(y_ext, p_ext))\n",
        "        ax.scatter(y_ext, p_ext, color=COLOR_EXT, alpha=0.8, edgecolors='k', s=90, marker='D',\n",
        "                   label=f\"External Test (Unseen)\\n$R^2$={r2_ext:.4f}, RMSEP={rmse_ext:.4f}\")\n",
        "\n",
        "    # Ideal Line\n",
        "    all_vals = np.concatenate([y_ncv, y_ext]) if y_ext is not None else y_ncv\n",
        "    min_val, max_val = all_vals.min(), all_vals.max()\n",
        "    buff = (max_val - min_val) * 0.05\n",
        "    ax.plot([min_val-buff, max_val+buff], [min_val-buff, max_val+buff], 'k--', lw=2)\n",
        "\n",
        "    apply_publication_style(ax, f\"Predicted vs Actual: {title_info}\", \"Reference Value\", \"Predicted Value\")\n",
        "    ax.legend(loc='upper left', fontsize=10, frameon=True)\n",
        "    return save_dual_format(fig, save_base)\n",
        "\n",
        "def plot_residuals(y_ncv, p_ncv, y_ext, p_ext, title_info, save_base):\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    res_ncv = y_ncv - p_ncv\n",
        "    std_ncv = np.std(res_ncv)\n",
        "    stud_ncv = res_ncv / std_ncv if std_ncv > 1e-9 else res_ncv\n",
        "    ax.scatter(p_ncv, stud_ncv, color=COLOR_NCV, alpha=0.6, s=60, label=\"Nested CV\")\n",
        "\n",
        "    if y_ext is not None:\n",
        "        res_ext = y_ext - p_ext\n",
        "        stud_ext = res_ext / std_ncv if std_ncv > 1e-9 else res_ext\n",
        "        ax.scatter(p_ext, stud_ext, color=COLOR_EXT, alpha=0.8, marker='D', s=70, label=\"External Test\")\n",
        "\n",
        "    ax.axhline(0, color='k', linestyle='--')\n",
        "    ax.axhline(3, color='r', linestyle=':', alpha=0.5)\n",
        "    ax.axhline(-3, color='r', linestyle=':', alpha=0.5)\n",
        "\n",
        "    apply_publication_style(ax, f\"Residuals: {title_info}\", \"Predicted Value\", \"Studentized Residuals\")\n",
        "    ax.legend()\n",
        "    return save_dual_format(fig, save_base)\n",
        "\n",
        "def plot_spectrum(X_df, selected_indices, title_info, save_base):\n",
        "    if X_df is None or X_df.empty: return None, None\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "    # Try to parse numeric wavelengths\n",
        "    try:\n",
        "        x_vals = pd.to_numeric(X_df.columns).to_numpy()\n",
        "        xlabel = \"Wavelength (nm)\"\n",
        "    except:\n",
        "        x_vals = np.arange(X_df.shape[1])\n",
        "        xlabel = \"Feature Index\"\n",
        "\n",
        "    mean_spec = X_df.mean(axis=0).to_numpy()\n",
        "    ax.plot(x_vals, mean_spec, color=COLOR_MEAN_SPECTRUM, label=\"Mean Spectrum\", linewidth=2)\n",
        "\n",
        "    if selected_indices is not None:\n",
        "        # Map back to original indices\n",
        "        idx_list = [i for i in selected_indices if i < len(x_vals)]\n",
        "        if idx_list:\n",
        "            ax.scatter(x_vals[idx_list], mean_spec[idx_list], color=COLOR_SELECTED_DOTS, s=40, zorder=5, label=\"Selected Features\")\n",
        "\n",
        "    apply_publication_style(ax, f\" {title_info}\", xlabel, \"Absorbance (AU)\")\n",
        "    ax.legend()\n",
        "    return save_dual_format(fig, save_base)\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 3: CORE LOGIC\n",
        "# ==============================================================================\n",
        "\n",
        "def load_data(path, target_col):\n",
        "    try: df = pd.read_excel(path)\n",
        "    except Exception as e: return None, None, str(e)\n",
        "\n",
        "    if target_col and target_col in df.columns:\n",
        "        y = df[target_col]; X = df.drop(columns=[target_col])\n",
        "    else:\n",
        "        y = df.iloc[:, 0]; X = df.iloc[:, 1:]\n",
        "\n",
        "    y = pd.to_numeric(y, errors='coerce')\n",
        "    mask = ~y.isna() & ~np.isinf(y)\n",
        "    X = X[mask]; y = y[mask]\n",
        "    X = X.select_dtypes(include=np.number).fillna(0)\n",
        "\n",
        "    if X.empty or y.empty: return None, None, \"Data is empty after cleaning.\"\n",
        "    return X, y, \"Success\"\n",
        "\n",
        "def get_model_config(max_pls_comps, n_samples):\n",
        "    \"\"\"\n",
        "    Returns search spaces matching Table S1 exactly.\n",
        "    \"\"\"\n",
        "    configs = []\n",
        "    # PLS: Integer [1, min(20, n-2)]\n",
        "    pls_comps = max(1, min(max_pls_comps, n_samples-2))\n",
        "    configs.append({\n",
        "        \"class\": PLSRegression, \"name\": \"PLS Regression\", \"fixed\": {\"scale\": False}, # Scaler handled by Pipeline\n",
        "        \"space\": [Integer(1, pls_comps, name='n_components')]\n",
        "    })\n",
        "    # Ridge: Log-uniform [1e-4, 1e3]\n",
        "    configs.append({\n",
        "        \"class\": Ridge, \"name\": \"Ridge Regression\", \"fixed\": {},\n",
        "        \"space\": [Real(1e-4, 1e3, prior='log-uniform', name='alpha')]\n",
        "    })\n",
        "    # ElasticNet: Alpha Log-uniform [1e-4, 10], L1 Uniform [0.01, 0.99]\n",
        "    configs.append({\n",
        "        \"class\": ElasticNet, \"name\": \"Elastic Net\", \"fixed\": {\"max_iter\": 5000, \"tol\": 1e-3},\n",
        "        \"space\": [Real(1e-4, 10, prior='log-uniform', name='alpha'), Real(0.01, 0.99, name='l1_ratio')]\n",
        "    })\n",
        "    # SVR: C Log-uniform [0.1, 1000], Gamma Log-uniform [1e-4, 10]\n",
        "    configs.append({\n",
        "        \"class\": SVR, \"name\": \"SVR (RBF)\", \"fixed\": {\"kernel\": \"rbf\"},\n",
        "        \"space\": [Real(0.1, 1000, prior='log-uniform', name='C'), Real(1e-4, 10, prior='log-uniform', name='gamma')]\n",
        "    })\n",
        "    return configs\n",
        "\n",
        "def objective_function(model_cls, fixed, space, X, y, cv_obj):\n",
        "    \"\"\"\n",
        "    Evaluates a model configuration.\n",
        "    CRITICAL: Creates a NEW Pipeline(Scaler -> Model) for every evaluation.\n",
        "    This ensures scaling is done inside the CV fold (No Leakage).\n",
        "    \"\"\"\n",
        "    def obj(**params):\n",
        "        global TOTAL_MODELS_TRAINED\n",
        "        TOTAL_MODELS_TRAINED += 1\n",
        "        all_params = {**fixed, **params}\n",
        "\n",
        "        # Instantiate base model\n",
        "        model = model_cls(**all_params)\n",
        "\n",
        "        # PLS Validity check\n",
        "        if model_cls == PLSRegression:\n",
        "            if all_params['n_components'] > X.shape[1]: return 1e12\n",
        "\n",
        "        # Create Pipeline: Scaling -> Model\n",
        "\n",
        "        pipe = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('est', model)\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            # Pass RAW X and y. Cross_val_score splits them, then Pipeline scales the split.\n",
        "            scores = cross_val_score(pipe, X, y, cv=cv_obj, scoring='r2', n_jobs=-1)\n",
        "            return -np.mean(scores)\n",
        "        except: return 1e12\n",
        "\n",
        "    return obj\n",
        "\n",
        "def run_optimization_pipeline(X_train, y_train, n_bo_p0, n_bo_p2, n_bo_p3, r2_cutoff, inner_cv_splits, max_pls, track_ablation=False):\n",
        "    ablation_log = []\n",
        "    n_samples_train = len(y_train)\n",
        "    inner_cv = KFold(n_splits=inner_cv_splits, shuffle=True, random_state=42)\n",
        "    n_feat = X_train.shape[1]\n",
        "\n",
        "    # --- Phase 0: Broad Search (No Pre-Scaling here, Pipeline handles it) ---\n",
        "    configs = get_model_config(max_pls, n_samples_train)\n",
        "    p0_results = []\n",
        "\n",
        "    for cfg in configs:\n",
        "        @use_named_args(cfg['space'])\n",
        "        def wrapper(**p): return objective_function(cfg['class'], cfg['fixed'], cfg['space'], X_train, y_train, inner_cv)(**p)\n",
        "\n",
        "        try:\n",
        "            # Fixed random\n",
        "            res = gp_minimize(wrapper, cfg['space'], n_calls=n_bo_p0, random_state=42)\n",
        "            p0_results.append({**cfg, \"params\": dict(zip([d.name for d in cfg['space']], res.x)), \"score\": -res.fun})\n",
        "        except: pass\n",
        "\n",
        "    if not p0_results: return None, []\n",
        "    p0_results.sort(key=lambda x: x['score'], reverse=True)\n",
        "    best_curr = p0_results[0]\n",
        "\n",
        "    # Build the best Phase 0 pipeline\n",
        "    m_base = best_curr['class'](**best_curr['fixed'], **best_curr['params'])\n",
        "    best_pipe = Pipeline([('scaler', StandardScaler()), ('est', m_base)])\n",
        "    best_pipe.fit(X_train, y_train) # Fit on full training fold\n",
        "\n",
        "    if track_ablation:\n",
        "        # Recalculate CV score for logging\n",
        "        cv_score = cross_val_score(best_pipe, X_train, y_train, cv=inner_cv, scoring='r2', n_jobs=-1).mean()\n",
        "        mp, fp = extract_params(best_pipe)\n",
        "        ablation_log.append({\n",
        "            \"Phase\": \"0 (Baseline)\", \"Model\": best_curr['name'], \"CV_R2\": cv_score, \"Feats\": n_feat,\n",
        "            \"ModelParams\": mp, \"FilterParams\": fp\n",
        "        })\n",
        "\n",
        "    best_state = {\n",
        "        \"model_name\": best_curr['name'], \"pipeline\": best_pipe, \"filter_desc\": \"None\",\n",
        "        \"indices\": list(range(n_feat)), \"score\": best_curr['score']\n",
        "    }\n",
        "\n",
        "    # --- Phase 1: Filter Selection (If R2 < threshold) ---\n",
        "    if best_state['score'] < r2_cutoff:\n",
        "        filters = [\n",
        "            (\"VarThresh\", VarianceThreshold(threshold=0.01)),\n",
        "            (\"Anova\", SelectKBest(f_regression)),\n",
        "            (\"Pearson\", SelectKBest(pearson_corr_score_func)),\n",
        "            (\"Spearman\", SelectKBest(spearman_corr_score_func))\n",
        "        ]\n",
        "        if ReliefF: filters.append((\"ReliefF\", ReliefF(n_neighbors=min(20, n_samples_train-1), n_jobs=1)))\n",
        "\n",
        "        k_options = sorted(list(set([int(n_feat * p) for p in [0.1, 0.3, 0.5]])))\n",
        "        if not k_options: k_options = [max(1, n_feat // 2)]\n",
        "\n",
        "        improved_p1 = False\n",
        "\n",
        "        for k in k_options:\n",
        "            for fname, fobj_template in filters:\n",
        "                try:\n",
        "                    fobj = sklearn.base.clone(fobj_template)\n",
        "                    if hasattr(fobj, 'k'): fobj.k = k\n",
        "                    elif hasattr(fobj, 'n_features_to_select'): fobj.n_features_to_select = k\n",
        "\n",
        "                    # Re-instantiate base model\n",
        "                    base_m = best_curr['class'](**best_curr['fixed'], **best_curr['params'])\n",
        "                    if isinstance(base_m, PLSRegression): base_m.n_components = min(base_m.n_components, k)\n",
        "\n",
        "                    # Build Pipeline: Scaler -> Filter -> Model\n",
        "                    pipe_cand = Pipeline([\n",
        "                        ('scaler', StandardScaler()),\n",
        "                        ('sel', fobj),\n",
        "                        ('est', base_m)\n",
        "                    ])\n",
        "\n",
        "                    scores = cross_val_score(pipe_cand, X_train, y_train, cv=inner_cv, scoring='r2', n_jobs=-1)\n",
        "                    mean_cv = np.mean(scores)\n",
        "\n",
        "                    if mean_cv > best_state['score']:\n",
        "                        improved_p1 = True\n",
        "                        pipe_cand.fit(X_train, y_train)\n",
        "\n",
        "                        # Extract indices from fitted pipeline\n",
        "                        supp = []\n",
        "                        if hasattr(pipe_cand.named_steps['sel'], 'get_support'):\n",
        "                            supp = pipe_cand.named_steps['sel'].get_support(indices=True).tolist()\n",
        "                        elif fname == \"ReliefF\": # ReliefF doesn't have get_support\n",
        "                             # For reporting only; the pipeline handles prediction correctly\n",
        "                             pass\n",
        "\n",
        "                        best_state = {\n",
        "                            \"model_name\": best_curr['name'], \"pipeline\": pipe_cand,\n",
        "                            \"filter_desc\": f\"{fname} (k={k})\", \"indices\": supp, \"score\": mean_cv\n",
        "                        }\n",
        "                except: pass\n",
        "\n",
        "        if track_ablation and improved_p1:\n",
        "            mp, fp = extract_params(best_state['pipeline'])\n",
        "            ablation_log.append({\n",
        "                \"Phase\": \"1 (Filter)\", \"Model\": best_state['model_name'], \"CV_R2\": best_state['score'],\n",
        "                \"Feats\": len(best_state['indices']) if best_state['indices'] else \"k=\"+str(fp.get('k', '?')),\n",
        "                \"ModelParams\": mp, \"FilterParams\": fp\n",
        "            })\n",
        "\n",
        "    return best_state, ablation_log\n",
        "\n",
        "def generate_pdf_report(stats, best_final_info, ablation_log, total_models, filename, duration_str):\n",
        "    try:\n",
        "        pdf = MarkdownPdf(toc_level=2)\n",
        "        md = f\"# AutoRegress: Comprehensive Analysis Report\\n\"\n",
        "        md += f\"**Date:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
        "        md += f\"**Execution Time:** {duration_str}\\n\"\n",
        "        md += f\"**Models Evaluated:** {total_models:,} (approx)\\n\\n\" # Added comma formatting\n",
        "\n",
        "        md += \"## 1. Performance Summary\\n\"\n",
        "        md += \"| Metric | Training (Fit) | Nested CV (Internal) | External Test (Unseen) |\\n|---|---|---|---|\\n\"\n",
        "        md += f\"| **RÂ²** | {stats['train_r2_m']:.4f} | {stats['cv_r2_m']:.4f} | **{stats['test_r2_m']:.4f}** |\\n\"\n",
        "        md += f\"| **RMSE** | {stats['train_rmse_m']:.4f} | {stats['cv_rmse_m']:.4f} | **{stats['test_rmse_m']:.4f}** |\\n\"\n",
        "\n",
        "\n",
        "        md += \"\\n## 2. Final Model Configuration\\n\"\n",
        "        md += f\"- **Algorithm:** {best_final_info['model_name']}\\n\"\n",
        "        md += f\"- **Filter Strategy:** {best_final_info['filter_desc']}\\n\"\n",
        "\n",
        "        if ablation_log:\n",
        "            final_log = ablation_log[-1]\n",
        "            md += \"\\n### Hyperparameters\\n\"\n",
        "            md += f\"- **Model Params:** `{final_log.get('ModelParams', 'N/A')}`\\n\"\n",
        "            md += f\"- **Filter Params:** `{final_log.get('FilterParams', 'N/A')}`\\n\"\n",
        "\n",
        "        md += \"\\n## 3. Optimization Trajectory (Ablation)\\n\"\n",
        "        md += \"| Phase | Best CV RÂ² | Active Features | Params |\\n|---|---|---|---|\\n\"\n",
        "        for p in ablation_log:\n",
        "            mp_str = str(p.get('ModelParams', ''))[:50] + \"...\" if len(str(p.get('ModelParams', ''))) > 50 else str(p.get('ModelParams', ''))\n",
        "            md += f\"| {p['Phase']} | {p['CV_R2']:.4f} | {p['Feats']} | `{mp_str}` |\\n\"\n",
        "\n",
        "        pdf.add_section(Section(md))\n",
        "        pdf.save(filename)\n",
        "    except Exception as e: print(f\"PDF Error: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 4: MAIN CONTROLLER\n",
        "# ==============================================================================\n",
        "\n",
        "def execute_autoregress(file_obj, target_col, test_row, outer_splits, inner_splits, max_pls,\n",
        "                        n_bo0, n_bo2, n_bo3, r2_cut, n_runs, progress=gr.Progress()):\n",
        "    global GLOBAL_PIPELINE, GLOBAL_FEATURE_NAMES, TOTAL_MODELS_TRAINED\n",
        "\n",
        "    start_time = time.time()\n",
        "    TOTAL_MODELS_TRAINED = 0\n",
        "\n",
        "    try:\n",
        "        if file_obj is None: return \"Please upload a file.\", None, None, None, None, None\n",
        "\n",
        "        sys_log = log_system_resources()\n",
        "        temp_dir = f\"gradio_runs/{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        X, y, msg = load_data(file_obj.name, target_col)\n",
        "        if X is None: return f\"Error: {msg}\", None, None, None, None, None\n",
        "\n",
        "        GLOBAL_FEATURE_NAMES = X.columns.tolist()\n",
        "        status_msg = sys_log\n",
        "\n",
        "        # --- REPEATED RUNS LOOP ---\n",
        "        agg_metrics = {k: [] for k in ['train_r2', 'train_rmse', 'train_mae', 'cv_r2', 'cv_rmse', 'cv_mae', 'test_r2', 'test_rmse', 'test_mae']}\n",
        "\n",
        "        best_run_score = -np.inf\n",
        "        best_overall_package = None\n",
        "\n",
        "        status_msg += f\"Starting {n_runs} Repeated Runs (Monte Carlo / Fixed Split)...\\n\"\n",
        "\n",
        "        for i in range(int(n_runs)):\n",
        "            progress((i+1)/n_runs, desc=f\"Run {i+1}/{n_runs}\")\n",
        "\n",
        "            # 1. SPLIT (Raw Data)\n",
        "            if test_row and str(test_row).strip():\n",
        "                try:\n",
        "                    split_idx = int(test_row) - 1\n",
        "                    X_opt, y_opt = X.iloc[:split_idx], y.iloc[:split_idx]\n",
        "                    X_ext, y_ext = X.iloc[split_idx:], y.iloc[split_idx:]\n",
        "                except: X_opt, X_ext, y_opt, y_ext = train_test_split(X, y, test_size=0.20, random_state=42+i)\n",
        "            else:\n",
        "                X_opt, X_ext, y_opt, y_ext = train_test_split(X, y, test_size=0.20, random_state=42+i)\n",
        "\n",
        "            # 2. Nested CV (Evaluation)\n",
        "            # We pass RAW X_tr, X_val. The optimization function builds a Pipeline(Scaler->Model).\n",
        "            outer_cv = KFold(n_splits=int(outer_splits), shuffle=True, random_state=42)\n",
        "            y_true_ncv = []\n",
        "            y_pred_ncv = []\n",
        "\n",
        "            for train_idx, val_idx in outer_cv.split(X_opt, y_opt):\n",
        "                X_tr, X_val = X_opt.iloc[train_idx], X_opt.iloc[val_idx]\n",
        "                y_tr, y_val = y_opt.iloc[train_idx], y_opt.iloc[val_idx]\n",
        "\n",
        "                # Run optimization on raw fold data\n",
        "                res, _ = run_optimization_pipeline(X_tr, y_tr, int(n_bo0), 0, 0, r2_cut, int(inner_splits), int(max_pls))\n",
        "\n",
        "                if res:\n",
        "                    # res['pipeline'] is already fitted on X_tr (and contains its own internal scaler)\n",
        "                    # We just predict on X_val (raw). The pipeline scales it automatically.\n",
        "                    p_val = res['pipeline'].predict(X_val)\n",
        "                    y_true_ncv.extend(y_val)\n",
        "                    y_pred_ncv.extend(p_val)\n",
        "\n",
        "            # 3. Final Model for this Run (Fit on full Optimization set)\n",
        "            final_res, ablation_log = run_optimization_pipeline(X_opt, y_opt, int(n_bo0), 0, 0, r2_cut, int(inner_splits), int(max_pls), track_ablation=True)\n",
        "            curr_pipe = final_res['pipeline']\n",
        "\n",
        "            # 4. Metrics\n",
        "            # CV\n",
        "            y_true_ncv, y_pred_ncv = np.array(y_true_ncv), np.array(y_pred_ncv)\n",
        "            run_cv_r2 = r2_score(y_true_ncv, y_pred_ncv)\n",
        "            run_cv_rmse = np.sqrt(mean_squared_error(y_true_ncv, y_pred_ncv))\n",
        "\n",
        "            # Train (Fit) - Pipeline handles scaling\n",
        "            p_opt = curr_pipe.predict(X_opt)\n",
        "            run_tr_r2 = r2_score(y_opt, p_opt)\n",
        "            run_tr_rmse = np.sqrt(mean_squared_error(y_opt, p_opt))\n",
        "\n",
        "            # Test (External) - Pipeline handles scaling\n",
        "            p_ext = curr_pipe.predict(X_ext)\n",
        "            run_te_r2 = r2_score(y_ext, p_ext)\n",
        "            run_te_rmse = np.sqrt(mean_squared_error(y_ext, p_ext))\n",
        "\n",
        "            # Store Metrics\n",
        "            agg_metrics['train_r2'].append(run_tr_r2); agg_metrics['train_rmse'].append(run_tr_rmse)\n",
        "            agg_metrics['cv_r2'].append(run_cv_r2); agg_metrics['cv_rmse'].append(run_cv_rmse)\n",
        "            agg_metrics['test_r2'].append(run_te_r2); agg_metrics['test_rmse'].append(run_te_rmse)\n",
        "\n",
        "            # Store best run for plotting\n",
        "            if run_te_r2 > best_run_score:\n",
        "                best_run_score = run_te_r2\n",
        "                GLOBAL_PIPELINE = curr_pipe\n",
        "                best_overall_package = {\n",
        "                    \"y_ncv\": y_true_ncv, \"p_ncv\": y_pred_ncv, \"y_ext\": y_ext, \"p_ext\": p_ext,\n",
        "                    \"res\": final_res, \"ablation\": ablation_log\n",
        "                }\n",
        "\n",
        "        # --- Final Reporting ---\n",
        "        stats = {k+\"_m\": np.mean(v) for k,v in agg_metrics.items()}\n",
        "        stats.update({k+\"_s\": np.std(v) for k,v in agg_metrics.items()})\n",
        "        stats['train_mae_m'] = 0; stats['cv_mae_m'] = 0; stats['test_mae_m'] = 0\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        duration_str = str(datetime.timedelta(seconds=int(elapsed)))\n",
        "        status_msg += f\"\\nTotal Time: {duration_str} | Models Evaluated: {TOTAL_MODELS_TRAINED}\\n\"\n",
        "\n",
        "        # Generate Artifacts\n",
        "        pdf_path = os.path.join(temp_dir, \"AutoRegress_Report.pdf\")\n",
        "        generate_pdf_report(stats, best_overall_package['res'], best_overall_package['ablation'], TOTAL_MODELS_TRAINED, pdf_path, duration_str)\n",
        "\n",
        "        # Plots\n",
        "        t1, p1 = plot_validation_results(best_overall_package['y_ncv'], best_overall_package['p_ncv'], best_overall_package['y_ext'], best_overall_package['p_ext'], \"Best Run\", os.path.join(temp_dir, \"perf\"))\n",
        "        t2, p2 = plot_residuals(best_overall_package['y_ncv'], best_overall_package['p_ncv'], best_overall_package['y_ext'], best_overall_package['p_ext'], \"Best Run\", os.path.join(temp_dir, \"resid\"))\n",
        "        t3, p3 = plot_spectrum(X, best_overall_package['res']['indices'], \"Selected Features\", os.path.join(temp_dir, \"spec\"))\n",
        "\n",
        "        zip_path = os.path.join(temp_dir, \"Results.zip\")\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "            for f in [t1, t2, t3, pdf_path]: zf.write(f, os.path.basename(f))\n",
        "\n",
        "        return status_msg, \"Analysis Complete.\", p1, p2, p3, zip_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"CRITICAL ERROR: {str(e)}\\n{traceback.format_exc()}\", None, None, None, None, None\n",
        "\n",
        "def predict_new_data(file_obj):\n",
        "    if GLOBAL_PIPELINE is None: return \"Error: You must train a model first.\", None\n",
        "    if file_obj is None: return \"Error: Please upload a file.\", None\n",
        "    try:\n",
        "        df = pd.read_excel(file_obj.name)\n",
        "        X_new = df.select_dtypes(include=np.number).fillna(0)\n",
        "        # Prediction is now simple: Pipeline handles scaling internally\n",
        "        preds = GLOBAL_PIPELINE.predict(X_new)\n",
        "        out_df = pd.DataFrame(preds, columns=[\"Predicted Value\"])\n",
        "        out_path = \"predictions.csv\"\n",
        "        out_df.to_csv(out_path, index=False)\n",
        "        return f\"Success! Predicted {len(preds)} samples.\", out_path\n",
        "    except Exception as e: return f\"Error: {str(e)}\", None\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 5: UI\n",
        "# ==============================================================================\n",
        "\n",
        "about_markdown = \"\"\"\n",
        "# AutoRegress: HPC Nested CV Edition\n",
        "\n",
        "**AutoRegress** is an automated framework designed to democratize high-performance spectral analysis. It abstracts away the complexity of training hundrads of models to deliver statistically rigorous results.\n",
        "\n",
        "## ðŸš€ Workflow\n",
        "1.  **Train:** Upload labeled data.\n",
        "    * *Manual Split:* Enter a row number to lock away future data.\n",
        "    * *Default Split:* Leave blank for a 20% Random Unseen Test Set.\n",
        "2.  **Optimize:** Uses **Bayesian Optimization** to tune:\n",
        "    * **Models:** PLS, Ridge, ElasticNet, SVR.\n",
        "    * **Filters:** VarianceThreshold, SelectKBest (ANOVA/Pearson/Spearman), ReliefF.\n",
        "3.  **Evaluate:** Reports metrics for **Training** , **Nested CV** , and **Test** (Unseen).\n",
        "\n",
        "## ðŸ’» HTC Justification\n",
        "The system tracks the **Total Models Trained**, calculated as:\n",
        "$$ Models = Outer_{Splits} \\\\times Inner_{Splits} \\\\times BO_{Calls} \\\\times Algorithm_{Count} $$\n",
        "This massive computational load justifies the use of Parallel Processing.\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    with gr.Tabs():\n",
        "        # TAB 1: TRAINING\n",
        "        with gr.TabItem(\"Train & Evaluate\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    file = gr.File(label=\"Training Data (.xlsx)\")\n",
        "                    target = gr.Textbox(label=\"Target Column (Optional - First Column)\")\n",
        "                    test_row = gr.Textbox(label=\"Test Set Start Row (Optional - Manual Split)\")\n",
        "\n",
        "                    with gr.Accordion(\"Advanced Settings\"):\n",
        "                        r2_cut = gr.Slider(0.9, 1.0, 0.995, label=\"R2 Cutoff\")\n",
        "                        n_runs = gr.Slider(1, 20, value=3, step=1, label=\"Repeated Runs (Robustness)\")\n",
        "                        outer_splits = gr.Slider(2, 10, value=5, step=1, label=\"Outer CV Splits\")\n",
        "                        inner_splits = gr.Slider(2, 10, value=3, step=1, label=\"Inner CV Splits\")\n",
        "\n",
        "                    with gr.Accordion(\"Model Settings\"):\n",
        "                        max_pls = gr.Slider(1, 20, value=10, step=1, label=\"Max PLS\")\n",
        "                        n_bo0 = gr.Slider(5, 50, value=15, step=1, label=\"BO Calls\")\n",
        "\n",
        "                    btn = gr.Button(\"Execute Hybrid Analysis\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column():\n",
        "                    html = gr.HTML(label=\"Results\")\n",
        "                    log = gr.Textbox(label=\"System & Run Log\", lines=10)\n",
        "                    with gr.Row():\n",
        "                        img1 = gr.Image(label=\"Performance Overview\")\n",
        "                        img2 = gr.Image(label=\"Residuals\")\n",
        "                    img3 = gr.Image(label=\"Spectrum\")\n",
        "                    pdf = gr.File(label=\"Download Report & PDF Figures (ZIP)\")\n",
        "\n",
        "            btn.click(execute_autoregress,\n",
        "                      inputs=[file, target, test_row, outer_splits, inner_splits, max_pls, n_bo0, n_bo0, n_bo0, r2_cut, n_runs],\n",
        "                      outputs=[log, html, img1, img2, img3, pdf])\n",
        "\n",
        "        # TAB 2: PREDICTION\n",
        "        with gr.TabItem(\"Predict New Data\"):\n",
        "            gr.Markdown(\"### Predict using the Final Model\")\n",
        "            p_file = gr.File(label=\"New Data (.xlsx) - Features Only\")\n",
        "            p_btn = gr.Button(\"Generate Predictions\", variant=\"primary\")\n",
        "            p_out = gr.Textbox(label=\"Status\")\n",
        "            p_csv = gr.File(label=\"Download Predictions\")\n",
        "            p_btn.click(predict_new_data, inputs=[p_file], outputs=[p_out, p_csv])\n",
        "\n",
        "        # TAB 3: ABOUT\n",
        "        with gr.TabItem(\"About\"):\n",
        "            gr.Markdown(about_markdown)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(\"gradio_runs\"): os.mkdir(\"gradio_runs\")\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cq_ILgQRsLbm",
        "outputId": "576f029a-2d09-480b-9873-33d978589c5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.12/dist-packages (0.10.2)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: skrebate in /usr/local/lib/python3.12/dist-packages (0.62)\n",
            "Requirement already satisfied: markdown-pdf in /usr/local/lib/python3.12/dist-packages (1.10)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (5.9.5)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (1.5.3)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.7.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scikit-optimize) (25.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: PyMuPDF>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from markdown-pdf) (1.26.7)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.12/dist-packages (from markdown-pdf) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py==3.0.0->markdown-pdf) (0.1.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.22)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.14)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2775885238.py:594: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ec1df606be43bb363b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ec1df606be43bb363b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}